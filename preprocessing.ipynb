{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='8px'> <center> Preprocessing </center> </font>\n",
    "<br>\n",
    "\n",
    "<font>Both user and Tappy data given in form .txt files are processed converted to .xlsx. Columns with categorical values are converted to one hot vectors and cleaned (Nan for the rows with miss matching values for particluar columns or in case of Unknown Format)\n",
    "</font>\n",
    "<br><br>\n",
    "\n",
    "<font>Main focus of this preprocessing by the end is to set dataset in such a way its easier to find time's correlation with parkisons\n",
    "</font>\n",
    "<br><br>\n",
    "\n",
    "<font size='3px' > There are 3 main sections: </font>\n",
    "<ol>\n",
    "    <li> All User details .txt file are processed and saved in one file called User_details.xlsx \n",
    "        <br> original size: 41.6 KB\n",
    "        <br>( 21.3 KB, 4 sec) </li>\n",
    "    <br>\n",
    "    <li> All tappy .txt files are processed and saved in form of .xlsx file \n",
    "        <br> Original size: 524 MB \n",
    "        <br>( 641 MB, ~2 hours ExcelWriter is slow on big datasets and also I was using jupyter python hence performace was affected ) \n",
    "    </li>\n",
    "    <br>\n",
    "    <li> Previously processed Tappy files are sorted based on timeline (date,time in asc order) and grouped later divided into sets based on date to be saved as &lt;User_unquieId&gt;_YYMMDD.xlsx this is done to make it easily feedable into RNN using a batch generator<br>\n",
    "    ( MB , ~ 2 hour)\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<font>\n",
    "    Files are processed and saved in this manner so they are easily accessible and loadable at the time of training and Sorting based on date and time helps in directly feeding it into RNN or anyother Squence model\n",
    "    </font>\n",
    "    <br>\n",
    "    <br>\n",
    "    <font color='red'>Error</font> in dataset: &nbsp; \" ZI1KGKLCD5_1612.txt \" is empty \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6px'> Part - I </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to user dataset\n",
    "user_dataset = \"dataset/users\"\n",
    "\n",
    "# user attribute\n",
    "user_list = ['user','BirthYear', 'Gender', 'Parkinsons', 'Tremors','DiagnosisYear', \n",
    "          'Sided', 'UPDRS', 'Impact', 'Levadopa', 'DA', 'MAOB', 'Other']\n",
    "\n",
    "# Setting attribute list for user dataframe\n",
    "user_dataframe = pd.DataFrame(columns  = user_list)\n",
    "\n",
    "# list of all the names of files in user directory\n",
    "user_file_list = os.listdir(user_dataset)\n",
    "\n",
    "# Adding value to user to dataset\n",
    "for i in range(len(user_file_list)):\n",
    "\n",
    "    # fetch filename \n",
    "    filename = user_file_list[i]\n",
    "\n",
    "    # name of user file is User_XXXXXXXXXX.txt\n",
    "    # we are only interest in 10 alphanumeric unique id\n",
    "    # convert '_' to '.' to ease the split function\n",
    "    # finally getting the id at index: 1\n",
    "    user_id = filename.replace('_','.').split('.')[1]\n",
    "    \n",
    "    # read the data\n",
    "    _data = pd.read_table(user_dataset+'/'+filename,header=None,sep=':')\n",
    "    \n",
    "    # convert second column to a list (Please cross check the format)\n",
    "    _data = _data[1].tolist()\n",
    "    \n",
    "    # Strip all the whitespace\n",
    "    for ind in range(len(_data)):\n",
    "        _data[ind] = _data[ind].replace(' ','')\n",
    "\n",
    "    # Insert it into user_dataframe with unique User Id as first column value\n",
    "    _data = [user_id] + _data\n",
    "\n",
    "    user_dataframe.loc[i] = _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4px'> Following shows the user attributes being converted to particular format </font>\n",
    "<ul>\n",
    "    <li> BirthYear : int </li><br>\n",
    "    <li> Gender : <br> \n",
    "         &nbsp; female -> 0 and male -> 1 </li><br>\n",
    "    <li> parkinsons :<br> \n",
    "         &nbsp; True -> 1 and False -> 0 </li><br>\n",
    "    <li> Tremors :<br> \n",
    "         &nbsp; True -> 1 and False -> 0 </li><br>\n",
    "    <li> DiagnosisYear : int ( if not given then Nan ) </li><br>\n",
    "    <li> Sided: Divided into 3 columns ( L_Sided, None_Sided, R_sided ) </li><br> \n",
    "    <li> UPDRS is scale having value from 0 - 4 convert it to one-hot encoding vector when not known all values are zero\n",
    "        <br> 0 = Absent.\n",
    "        <br> 1 = Slight and infrequently present.\n",
    "        <br> 2 = Mild in amplitude and persistent. Or moderate in amplitude, but only intermittently present.\n",
    "        <br> 3 = Moderate in amplitude and present most of the time.\n",
    "        <br> 4 = Marked in amplitude and present most of the time. \n",
    "    </li>\n",
    "    <br>\n",
    "    <li> Impact ( Mild/Medium/Severe ) to one-hot encoding vector  when not known all values are zero </li><br>\n",
    "    <li> Levadopa:<br> \n",
    "         &nbsp; True -> 1 and False -> 0 </li><br>\n",
    "    <li> DA:<br> \n",
    "         &nbsp; True -> 1 and False -> 0 </li><br>\n",
    "    <li> MAOB:<br> \n",
    "         &nbsp; True -> 1 and False -> 0 </li><br>\n",
    "    <li> Other:<br> \n",
    "         &nbsp; True -> 1 and False -> 0 </li>\n",
    " </ul>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['user','BirthYear', 'Gender', 'Parkinsons', 'Tremors','DiagnosisYear', 'Sided', 'UPDRS',\n",
    "#  'Impact', 'Levadopa', 'DA', 'MAOB', 'Other']\n",
    "\n",
    "# converting  BirthYear and  DiagnosisYear to numeric\n",
    "user_dataframe['BirthYear'] = pd.to_numeric(user_dataframe['BirthYear'],downcast=\"unsigned\", errors =\"coerce\")\n",
    "user_dataframe['DiagnosisYear'] = pd.to_numeric(user_dataframe['DiagnosisYear'],downcast=\"unsigned\", errors =\"coerce\")\n",
    "\n",
    "# Gender: String replacing it with int\n",
    "user_dataframe['Gender'] = user_dataframe['Gender'].replace({\"Female\":0,\"Male\":1})\n",
    "\n",
    "# Replace True with 1 and False with 0 keeping int dtype\n",
    "for i in ['Parkinsons', 'Tremors','Levadopa', 'DA', 'MAOB', 'Other']:\n",
    "    user_dataframe[i] = user_dataframe[i].replace({\"True\":1,\"False\":0})\n",
    "\n",
    "# Mapping cleans the cloumns making any other avlue present to Nan\n",
    "# Split the column into all the k columns (having k categories) \n",
    "# having drop_first = False gives us k columns hot encoding and not k-1\n",
    "user_dataframe['Impact'] = user_dataframe['Impact'].map({'Medium':'Medium','Mild':'Mild','Severe':'Severe'})\n",
    "user_dataframe = pd.get_dummies(user_dataframe, columns=['Impact'], drop_first=False)\n",
    "\n",
    "# Creates two columns Sided_R and Sided_L\n",
    "user_dataframe = pd.get_dummies(user_dataframe, columns=['Sided'], drop_first=False)\n",
    "\n",
    "# 'Dont know' is changed to Nan\n",
    "user_dataframe['UPDRS'] = user_dataframe['UPDRS'].map({'0':0,'1':1,'2':2,'3':3,'4':4})\n",
    "user_dataframe = pd.get_dummies(user_dataframe, columns=['UPDRS'], drop_first=False)\n",
    "\n",
    "\n",
    "# Save it for later use\n",
    "if os.path.exists('processed_dataset/User_details.xlsx'):\n",
    "    os.remove('processed_dataset/User_details.xlsx')\n",
    "\n",
    "writer = pd.ExcelWriter('processed_dataset/User_details.xlsx')\n",
    "user_dataframe.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6px'> Part - II </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to tappy dataset\n",
    "tappy_dataset = \"dataset/TappyData/\"\n",
    "\n",
    "# tappy data attribute\n",
    "tappy_header = ['user','date','time','Hand_Kpressed',\n",
    "                'Hold_time','Direction','Latency_time','Flight_time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all the files in directory\n",
    "tappy_file_list = os.listdir(tappy_dataset)\n",
    "\n",
    "# iterator through tappy data\n",
    "for i in tappy_file_list[:]:\n",
    "    \n",
    "    # Print Filename currently working on\n",
    "    print(i)\n",
    "    \n",
    "    userId_Tappy_df = pd.DataFrame(columns=tappy_header)\n",
    "    \n",
    "    \n",
    "    _data = pd.read_table(tappy_dataset+i,header=None,sep='\\t',error_bad_lines=False,low_memory=False)\n",
    "    _data = _data.drop(_data.columns[8],axis=1)\n",
    "    \n",
    "    \n",
    "    userId_Tappy_df['user'] = _data[0]\n",
    "\n",
    "    userId_Tappy_df['date'] = pd.to_datetime(_data[1],format=\"%y%m%d\",errors='coerce')        \n",
    "    # In Case we need Date in terms of 3 columns year,month,day (make sure to add columns to dataframe first)\n",
    "    #\n",
    "    # _data[1] = pd.to_datetime(_data[1],format=\"%y%m%d\")\n",
    "    # userId_Tappy_df['year'] = pd.DatetimeIndex(_data[1]).year\n",
    "    # userId_Tappy_df['month'] = pd.DatetimeIndex(_data[1]).month\n",
    "    # userId_Tappy_df['day'] = pd.DatetimeIndex(_data[1]).day\n",
    "\n",
    "\n",
    "    userId_Tappy_df['time'] = pd.to_datetime(_data[2],errors='coerce').dt.time\n",
    "    # In Case we need Time in terms of 3 columns hour,minute,second (make sure to add columns to dataframe first)\n",
    "    #\n",
    "    #userId_Tappy_df['hour'] = pd.to_datetime(_data[2]).dt.hour\n",
    "    #userId_Tappy_df['minute'] = pd.to_datetime(_data[2]).dt.minute\n",
    "    #userId_Tappy_df['second'] = pd.to_datetime(_data[2]).dt.second\n",
    "\n",
    "\n",
    "    userId_Tappy_df['Hand_Kpressed'] = _data[3].map({'L':'L','R':'R','S':'S'})\n",
    "    userId_Tappy_df = pd.get_dummies(userId_Tappy_df, columns=['Hand_Kpressed'], drop_first=False)\n",
    "    #\n",
    "    # This is needed becasuse some datasets are so small that they dont have all the direction cases hence\n",
    "    # get_dummies is not sufficient to handle such event\n",
    "    #\n",
    "    for index in ['L','R','S']:\n",
    "        if 'Hand_Kpressed_'+ index not in userId_Tappy_df:\n",
    "            userId_Tappy_df['Hand_Kpressed_'+ index] = 0\n",
    "\n",
    "\n",
    "    userId_Tappy_df['Hold_time'] = pd.to_numeric(_data[4],downcast='float',errors='coerce')\n",
    "\n",
    "\n",
    "    # One Hot Vector of Direction\n",
    "    userId_Tappy_df['Direction'] = _data[5].map({'LR':'LR','RR':'RR','SR':'SR',\n",
    "                                                 'LL':'LL','RL':'RL','SL':'SL',\n",
    "                                                 'LS':'LS','RS':'RS','SS':'SS'})\n",
    "    userId_Tappy_df = pd.get_dummies(userId_Tappy_df, columns=['Direction'], drop_first=False)\n",
    "    #\n",
    "    # In case of All 9 columns are not formed  ['LR','LL','LS','RR','RL','RS','SR','SL','SS']\n",
    "    # create the column that dont exists and column add zero to it\n",
    "    #\n",
    "    # This is needed becasuse some datasets are so small that they dont have all the direction cases hence\n",
    "    # get_dummies is not sufficient to handle such event\n",
    "    #\n",
    "    for index in ['LR','LL','LS','RR','RL','RS','SR','SL','SS']:\n",
    "        if 'Direction_'+ index not in userId_Tappy_df:\n",
    "            userId_Tappy_df['Direction_'+ index] = 0\n",
    "\n",
    "    userId_Tappy_df['Latency_time'] = pd.to_numeric(_data[6],downcast='float',errors='coerce')\n",
    "\n",
    "    userId_Tappy_df['Flight_time'] = pd.to_numeric(_data[7],downcast='float',errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "    name = i.split('.')[0]\n",
    "    if os.path.exists( 'processed_dataset/tappy_data/'+ name + '.xlsx'):\n",
    "        pass #os.remove( 'processed_dataset/tappy_data/'+ name + '.xlsx')\n",
    "    else:\n",
    "        writer = pd.ExcelWriter( 'processed_dataset/tappy_data/'+ name + '.xlsx')\n",
    "        userId_Tappy_df.to_excel(writer,'Sheet1')\n",
    "        writer.save()\n",
    "    \n",
    "print(\"\\n Completed ! \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6px'> Part - III </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_tappy = \"processed_dataset/tappy_data/\"\n",
    "furtherP_tappydata = \"processed_dataset/furtherProcessed_tappy_data/\"\n",
    "\n",
    "tappy_list = os.listdir(processed_tappy)\n",
    "\n",
    "\n",
    "# map all the files related to specific user \n",
    "user_tappy = {}\n",
    "for i in range(len(tappy_list)):\n",
    "    \n",
    "    # name of file gives us XXXXXXXXXX_YYMM.xlsx\n",
    "    filename = tappy_list[i]\n",
    "    \n",
    "    df_excel = pd.read_excel(processed_tappy+filename)\n",
    "    \n",
    "    # group and divided by date and later sort\n",
    "    gs = df_excel.groupby('date')\n",
    "    for indv in gs.groups:\n",
    "        \n",
    "        # get the file in format of  XXXXXXXXXX_YYMM\n",
    "        nFilename = filename.split('.')[0]\n",
    "        new_df = gs.get_group(indv)\n",
    "        \n",
    "        # sort by time just in case its not sorted \n",
    "        new_df.sort_values(by=['time'])\n",
    "        \n",
    "        # new_df.iloc[0]['date'] return timestamp in YYYY-MM-DD HH:MM:SS format\n",
    "        # extract only day\n",
    "        day = new_df.iloc[0]['date'].day\n",
    "        nFilename = furtherP_tappydata + nFilename + str(day) + '.xlsx'\n",
    "        \n",
    "        if os.path.exists(nFilename):\n",
    "            print('\\n\\n'+ nFilename + ' already exists\\n\\n')\n",
    "        else:  \n",
    "            # write the new dataframe to excel\n",
    "            writer = pd.ExcelWriter(nFilename)\n",
    "            new_df.to_excel(writer,'Sheet1')\n",
    "            writer.save()\n",
    "        \n",
    "        print(nFilename + ' written successfully')\n",
    "    \n",
    "print('\\n\\n Completed !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6px'> Part - IV </font> (Additional)\n",
    "<br>\n",
    "<font>\n",
    "    TO Group all the users dataset into a single file\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tappy = \"processed_dataset/tappy_data/\"\n",
    "furtherP_tappydata = \"processed_dataset/furtherProcessed_tappy_data/\"\n",
    "\n",
    "tappy_list = os.listdir(processed_tappy)\n",
    "\n",
    "\n",
    "# map all the files related to specific user \n",
    "user_tappy = {}\n",
    "for i in range(len(tappy_list)):\n",
    "    \n",
    "    # name of file gives us XXXXXXXXXX_YYMM.xlsx\n",
    "    filename = tappy_list[i]\n",
    "    \n",
    "    # Extracting the user_id\n",
    "    user_id = filename.replace('_','.').split('.')[0]\n",
    "    \n",
    "    # Adding all the files as list to user_id as key \n",
    "    if user_id in user_tappy.keys():\n",
    "        user_tappy[user_id].append(filename)\n",
    "    else:\n",
    "        user_tappy.update({user_id:[filename]})\n",
    "        \n",
    "for i in user_tappy.keys():\n",
    "    \n",
    "    print(user_tappy[i])\n",
    "    \n",
    "    # write it out\n",
    "    file = furtherP_tappydata+ i+\".xlsx\"\n",
    "    \n",
    "    if os.path.exists(file):\n",
    "        continue#os.remove(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Start writing from\n",
    "    startrow=0\n",
    "    for j in user_tappy[i]:\n",
    "        writer = pd.ExcelWriter(file)\n",
    "        #excel_names = user_tappy[i]\n",
    "        excel = pd.ExcelFile(processed_tappy+j)\n",
    "        #excels = [pd.ExcelFile(processed_tappy+name) for name in excel_names]\n",
    "        #frames = [x.parse(x.sheet_names[0], header=None,index_col=None) for x in excels]\n",
    "\n",
    "        # delete the first row for all frames except the first\n",
    "        # i.e. remove the header row -- assumes it's the first\n",
    "        \n",
    "        frames = excel.parse(excel.sheet_names[0], header=None,index_col=None)\n",
    "        #frames[1:] = [df[1:] for df in frames[1:]]\n",
    "\n",
    "        #print(frames)\n",
    "        header = frames.iloc[0].tolist()[1:]\n",
    "\n",
    "        # print(header)\n",
    "        # concatenate them..\n",
    "        #combined = pd.concat(frames)\n",
    "        #combined = combined.drop(columns=[0])\n",
    "\n",
    "        df = pd.DataFrame(columns=header)\n",
    "        \n",
    "        for ind,v in enumerate(header):\n",
    "            df[v] = frames[ind+1].iloc[1:]\n",
    "\n",
    "        \n",
    "        if startrow != 0:\n",
    "            df.to_excel(writer,'Sheet1',startrow=startrow,header=False,index= False )\n",
    "        else:\n",
    "            df.to_excel(writer,'Sheet1',startrow=startrow, index = False)\n",
    "            \n",
    "        startrow= startrow + len(df)\n",
    "    \n",
    "        writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6px'> Part - V </font>Error Checking\n",
    "<br>\n",
    "<font>\n",
    "    In the dataset of 622 tappy files in that some data belong to group of 49 users whose user details is not given \n",
    "    ( I.e birthyear , or whether they have parkinsons or they dont ) And in User details of 277 user their are 10 \n",
    "    users whose Tappy data is not given.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Checking\n",
    "import os\n",
    "\n",
    "tappy_dataset = \"dataset/TappyData/\"\n",
    "user_dataset = \"dataset/users/\"\n",
    "\n",
    "user_file_list = os.listdir(user_dataset)\n",
    "user_file_list = [w.replace('.txt', '') for w in user_file_list]\n",
    "user_file_list = [w.replace('User_', '') for w in user_file_list]\n",
    "\n",
    "\n",
    "tappy_dataset_list = os.listdir(tappy_dataset)\n",
    "\n",
    "tappy_dataset_list = [w.replace('.txt', '') for w in tappy_dataset_list]\n",
    "tappy_dataset_list = [w.split('_')[0] for w in tappy_dataset_list]\n",
    "\n",
    "\n",
    "c=1\n",
    "for i in set(tappy_dataset_list):\n",
    "    if i not in set(user_file_list):\n",
    "        print(i,\"   c:\",c)\n",
    "        c = c+1\n",
    "\n",
    "c =1\n",
    "print(\"\\n\\n\\n\")\n",
    "for i in set(user_file_list):\n",
    "    if i not in set(tappy_dataset_list):\n",
    "        print(i,\"   c:\",c)\n",
    "        c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
